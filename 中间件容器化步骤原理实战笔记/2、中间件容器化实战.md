## 中间件容器化实战

### 一、Operator模板信息

- Operator模板信息、其他的Operator模板可以在此寻找，都有部署文档的

```sh
Operator官方模板：https://github.com/operator-framework/awesome-operators
```

```sh
# 注意查看Table of Contents那，有讲存储、secret啥的
Redis Cluster Operator: https://github.com/ucloud/redis-cluster-operator
```



### 二、使用Operator部署Redis集群【测试、生产不能照搬这文档】

#### 2.1、下载源码

```sh
git clone https://github.com/ucloud/redis-cluster-operator.git
```

#### 2.2、创建Operator

```sh
cd redis-cluster-operator/
kubectl create -f deploy/crds/redis.kun_distributedredisclusters_crd.yaml
kubectl create -f deploy/crds/redis.kun_redisclusterbackups_crd.yaml
kubectl create ns infra
kubectl create -f deploy/service_account.yaml -n infra
kubectl create -f deploy/namespace/role.yaml -n infra
kubectl create -f deploy/namespace/role_binding.yaml -n infra
kubectl create -f deploy/namespace/operator.yaml -n infra

# 查看部署状态
[root@k8s-master01 redis-cluster-operator]# kubectl get po -n infra 
NAME                                      READY   STATUS    RESTARTS   AGE
redis-cluster-operator-675ccbc697-hvg59   1/1     Running   0          3m39s
```

#### 2.3、创建Redis集群

```sh
# Namespace级别的需要更改配置
# 【可选】提示：如果集群规模不大，资源少，可以自定义资源，把请求的资源降低

[root@k8s-master01 redis-cluster-operator]# vim deploy/example/custom-resources.yaml 
# redis.kun/scope: cluster-scoped 这行注释掉
kubectl create -f deploy/example/custom-resources.yaml -n infra

# 查看集群状态
[root@k8s-master01 redis-cluster-operator]# kubectl get distributedrediscluster -n infra
NAME                              MASTERSIZE   STATUS    AGE
example-distributedrediscluster   3            Scaling   22s

[root@k8s-master01 redis-cluster-operator]# kubectl get po -n infra 
NAME                                      READY   STATUS    RESTARTS   AGE
drc-example-distributedrediscluster-0-0   1/1     Running   0          4m35s
drc-example-distributedrediscluster-0-1   1/1     Running   0          3m12s
drc-example-distributedrediscluster-1-0   1/1     Running   0          4m34s
drc-example-distributedrediscluster-1-1   1/1     Running   0          3m35s
drc-example-distributedrediscluster-2-0   1/1     Running   0          4m35s
drc-example-distributedrediscluster-2-1   1/1     Running   0          3m13s

[root@k8s-master01 redis-cluster-operator]# kubectl get DistributedRedisCluster -n infra 
NAME                              MASTERSIZE   STATUS    AGE
example-distributedrediscluster   3            Healthy   19m
```

#### 2.4、如果想再起一个集群

```sh
# 执行以下命令即可
kubectl create ns infra1
kubectl create -f deploy/service_account.yaml -n infra1
kubectl create -f deploy/namespace/role.yaml -n infra1
kubectl create -f deploy/namespace/role_binding.yaml -n infra1
kubectl create -f deploy/namespace/operator.yaml -n infra1
kubectl create -f deploy/example/custom-resources.yaml -n infra1
```

#### 2.5、数据持久化【注意deploy/example这些配置文件都是起同一个集群的，就是功能不同而已】

```sh
# 更改这个文件即可deploy/example/persistent.yaml
kubectl create -f deploy/example/persistent.yaml
```

#### 2.6、扩容和卸载Redis集群

```sh
# 扩容Redis集群
grep "master" deploy/example/redis.kun_v1alpha1_distributedrediscluster_cr.yaml
masterSize: 4

# 注意这使用哪个apply 就用修改哪个
kubectl replace -f deploy/example/redis.kun_v1alpha1_distributedrediscluster_cr.yaml -n infra

[root@k8s-master01 redis-cluster-operator]# kubectl edit DistributedRedisCluster -n infra example-distributedrediscluster 
```

```sh
# 卸载集群
kubectl delete -f deploy/example/custom-resources.yaml -n infra
kubectl delete -f deploy/cluster/operator.yaml -n infra
kubectl delete -f deploy/cluster/cluster_role_binding.yaml -n infra
kubectl delete -f deploy/cluster/cluster_role.yaml -n infra
kubectl delete -f deploy/service_account.yaml -n infra
kubectl delete -f deploy/crds/redis.kun_redisclusterbackups_crd.yaml -n infra
kubectl delete -f deploy/crds/redis.kun_distributedredisclusters_crd.yaml -n infra
```



### 三、Helm部署Zookeeper集群 

#### 3.1、helm准备

```sh
# Helm客户端安装文档
https://helm.sh/docs/intro/install/

# 添加bitnami和官方helm仓库：
helm repo add bitnami https://charts.bitnami.com/bitnami
helm repo add stable https://charts.helm.sh/stable

# 更新仓库
helm  repo update
```

#### 3.1、部署Zookeeper、Kafka集群

```sh
# sc
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: infra-nfs-zk
provisioner: fuseim.pri/ifs
parameters:
  archiveOnDelete: "false"   # 设置为"false"时删除PVC不会保留数据,"true"则保留数据
  
# pvc
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: pvc-zk
  namespace: infra
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 2Gi
  storageClassName: infra-nfs-zk
```



- 安装方式一：先下载后安装

```sh
# 查看版本
[root@k8s-master01 helm]# helm search repo zookeeper
NAME                            CHART VERSION   APP VERSION     DESCRIPTION                                       
bitnami/zookeeper               10.2.3          3.8.0           Apache ZooKeeper provides a reliable, centraliz...
bitnami/dataplatform-bp1        12.0.2          1.0.1           DEPRECATED This Helm chart can be used for the ...
bitnami/dataplatform-bp2        12.0.5          1.0.1           DEPRECATED This Helm chart can be used for the ...
bitnami/kafka                   19.0.0          3.3.1           Apache Kafka is a distributed streaming platfor...
bitnami/schema-registry         6.0.0           7.2.2           Confluent Schema Registry provides a RESTful in...
bitnami/solr                    6.2.2           9.0.0           Apache Solr is an extremely powerful, open sour...

# 查看zookeeper包的历史版本
helm search repo zookeeper -l

# pull 包
helm pull bitnami/zookeeper

# 解压
[root@k8s-master01 helm]# tar -xf zookeeper-10.2.3.tgz  && cd zookeeper/

# 更改配置文件
# sc name
persistence.storageClass:"infra-nfs-zk"
dataLogDir.existingClaim: "pvc-zk"
replicaCount: 3
# tls.client.enabled: false 默认关闭

# 修改values.yaml相应配置：副本数、auth、持久化
[root@k8s-master01 zookeeper]# helm install -n infra zookeeper .
NAME: zookeeper
LAST DEPLOYED: Wed Oct 19 23:01:23 2022
NAMESPACE: infra
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
CHART NAME: zookeeper
CHART VERSION: 10.2.3
APP VERSION: 3.8.0

** Please be patient while the chart is being deployed **

ZooKeeper can be accessed via port 2181 on the following DNS name from within your cluster:

    zookeeper.infra.svc.cluster.local

To connect to your ZooKeeper server run the following commands:

    export POD_NAME=$(kubectl get pods --namespace infra -l "app.kubernetes.io/name=zookeeper,app.kubernetes.io/instance=zookeeper,app.kubernetes.io/component=zookeeper" -o jsonpath="{.items[0].metadata.name}")
    kubectl exec -it $POD_NAME -- zkCli.sh

To connect to your ZooKeeper server from outside the cluster execute the following commands:

    kubectl port-forward --namespace infra svc/zookeeper 2181:2181 &
    zkCli.sh 127.0.0.1:2181
    
# 查看部署结果
[root@k8s-master01 helm]# kubectl get po -n infra 
NAME                                      READY   STATUS    RESTARTS   AGE
zookeeper-0                               1/1     Running   0          3m49s
zookeeper-1                               1/1     Running   0          3m53s
zookeeper-2                               1/1     Running   0          3m49s
```

- 安装方式二：直接安装kafka

```sh
[root@k8s-master01 helm]# helm install kafka1 bitnami/kafka --set zookeeper.enabled=false --set replicaCount=3 --set externalZookeeper.servers=zookeeper --set persistence.enabled=false -n infra
NAME: kafka1
LAST DEPLOYED: Wed Oct 19 23:34:33 2022
NAMESPACE: infra
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
CHART NAME: kafka
CHART VERSION: 19.0.0
APP VERSION: 3.3.1

** Please be patient while the chart is being deployed **

Kafka can be accessed by consumers via port 9092 on the following DNS name from within your cluster:

    kafka1.infra.svc.cluster.local

Each Kafka broker can be accessed by producers via port 9092 on the following DNS name(s) from within your cluster:

    kafka1-0.kafka1-headless.infra.svc.cluster.local:9092
    kafka1-1.kafka1-headless.infra.svc.cluster.local:9092
    kafka1-2.kafka1-headless.infra.svc.cluster.local:9092

To create a pod that you can use as a Kafka client run the following commands:

    kubectl run kafka1-client --restart='Never' --image docker.io/bitnami/kafka:3.3.1-debian-11-r1 --namespace infra --command -- sleep infinity
    kubectl exec --tty -i kafka1-client --namespace infra -- bash

    PRODUCER:
        kafka-console-producer.sh \
            
            --broker-list kafka1-0.kafka1-headless.infra.svc.cluster.local:9092,kafka1-1.kafka1-headless.infra.svc.cluster.local:9092,kafka1-2.kafka1-headless.infra.svc.cluster.local:9092 \
            --topic test

    CONSUMER:
        kafka-console-consumer.sh \
            
            --bootstrap-server kafka1.infra.svc.cluster.local:9092 \
            --topic test \
            --from-beginning
            
# 查看部署结果
[root@k8s-master01 ~]# kubectl get po -n infra 
NAME                                      READY   STATUS              RESTARTS   AGE
kafka1-0                                  1/1     Running             0          14m
kafka1-1                                  0/1     Running             0          14m
kafka1-2                                  1/1     Running             0          14m
```

```sh
# Kafka验证
kubectl run kafka-client --restart='Never' --image docker.io/bitnami/kafka:2.8.0-debian-10-r30 --namespace
public-service --command -- sleep infinity
kubectl exec --tty -i kafka-client --namespace public-service -- bash

# 生产者:
kafka-console-producer.sh \
--broker-list kafka-0.kafka-headless.public-service.svc.cluster.local:9092,kafka-1.kafka-headless.public-
service.svc.cluster.local:9092,kafka-2.kafka-headless.public-service.svc.cluster.local:9092 \
--topic test

# 消费者:
kafka-console-consumer.sh \
--bootstrap-server kafka.public-service.svc.cluster.local:9092 \
--topic test \
--from-beginning
```

